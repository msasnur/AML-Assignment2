---
title: "IMDB-RNN-Assignment 2"
author: "msasnur"
output: html_document

---


```{r}
library(keras)


#Reading IMDB Dataset
imdb <- dataset_imdb(num_words = 10000)
c(c(x_train,y_train),c(x_test,y_test)) %<-% imdb

maxlen <- 150 #Sequence length set to 150
max_words <- 10000 #Top 10000 words choosen

#Turning the list of integers into a 2D integer tensor shape
x_train <- pad_sequences(x_train,maxlen = maxlen) 
x_test <- pad_sequences(x_test,maxlen=maxlen)

#Creating a training data with 100 samples
set.seed(1234)
train_index <- sample(1:nrow(x_train),100)
train_data <- x_train[train_index,]
train_label <- y_train[train_index]
traVal_data <- x_train[-train_index,]
traVal_label <- y_train[-train_index]

#Creating the validation data with 10000 samples
set.seed(1234)
valid_index <- sample(1:nrow(traVal_data),10000)
valid_data <- traVal_data[valid_index,] 
valid_label <- traVal_label[valid_index]

#Using an embedding layer and classifier on the IMDB dataset
model <- keras_model_sequential() %>% layer_embedding(input_dim = 10000,output_dim = 3,input_length = maxlen) %>% 
  layer_flatten() %>% layer_dense(units=1,activation = "sigmoid")
model %>% compile(optimizer = "rmsprop",loss = "binary_crossentropy",metrics=c("acc"))

history <- model %>% fit(train_data,train_label,epochs=10,batch_size=32,validation_data = list(valid_data,valid_label))
# Ploting for Accuracy and Loss function of the model
plot(history)
```

In this plot, validation accuracy is 70%. First 150 words in every review for 100 samples is considered

```{r}
# Evaluating the test dataset 
model %>% fit(
  train_data,
  train_label,
  epochs = 2,
  batch_size = 20)
result <- model %>%  evaluate(x_test,y_test)
result 
# Test Acuuracy is 51%

# Word index of the IMDB dataset 
word_index_1 <- dataset_imdb_word_index()

# Parsing the GloVe word-embeddings file
glove_dir = 'C:/Users/arjunsasnur/Desktop/glove.6B'
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))

embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)) {
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}
cat("Found", length(embeddings_index), "word vectors.\n")


# Preparing the GloVe word-embeddings matrix
embedding_dim <- 100
embedding_matrix <- array(0, c(max_words, embedding_dim))
for (word in names(word_index_1)) {
  index <- word_index_1[[word]]
  if (index < max_words) {
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector))
      embedding_matrix[index+1,] <- embedding_vector
  }
}

# Model construction
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_words, output_dim = embedding_dim,input_length = maxlen) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

# Loading pretrained word embeddings into the embedding layer
get_layer(model, index = 1) %>%
  set_weights(list(embedding_matrix)) %>%
  freeze_weights()

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

history1 <- model %>% fit(
  train_data, train_label,
  epochs = 20,
  batch_size = 32,
  validation_data = list(valid_data , valid_label)
)
plot(history1)
```

From above plot, the validaiton accuracy is 50% with 100 samples in the training dataset.This model overfits with small number of traning samples.

In this model, peformance is solely dependent on the small group of sample that's choosen.

```{r}
save_model_weights_hdf5(model, "pre_trained_glove_model.h5")
```